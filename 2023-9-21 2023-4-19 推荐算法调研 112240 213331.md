## 推荐算法调研

### 一 分类

​		**1.协同过滤推荐**：用户群过去的行为预测现在的喜好

​					输入：“用户—物体”评分矩阵

​					输出：用户对某个物体的喜爱程度评分；或者是推荐物品列表

​		又可细分为基于邻域的推荐和基于模型的推荐：

​					**基于邻域**：

​						基于用户的协同过滤：给用户推荐和他兴趣相似的其他用户喜欢的物品，

​								首先找到相似用户（比如皮尔逊相关度），然后对其他相似用户评分过的物品的评分进行相似度加权

​								缺点：用户兴趣相似度矩阵成本大，不适于大规模用户场景；

​											用户之间相似度不稳定，更新维护成本很高；

​											该种方法难以对推荐结果做出解释

​								适用场景：新闻推荐，因为新闻更新很快

​						基于物品的协同过滤：给用户推荐和他之前喜欢的物品相似的物品

​								 步骤：1.计算物品之间的相似度

​											2.根据物品的相似度和用户的历史行为给用户生成推荐列表。

​								缺点： 需要维护一个物品相似度矩阵，随着物品数目增多，维护物品相似度矩阵的代价越大。

​								适用场景：图书电影网站，因为这里的物品更新较慢；

​					**基于模型的推荐**：

​						 根据用户与物品/其他用户的历史相互作用来建模（比如图模型）

​						基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。

​							

​				协同过滤推荐的优缺点：

​						用户和物品的冷启动；没有考虑情景的差异；不擅长推荐小众喜好

​				

​			**2.基于内容推荐**

1、给出物品表示：为每个物品抽取出一些特征来表示此物品；

2、学习用户偏好：利用一个用户过去喜欢（及不喜欢）的物品的特征数据，来学习出此用户偏好；（相当于比起协同过滤多了特征表示这一步）
3、生成推荐列表：根据候选物品表示和用户偏好，为该用户生成其最可能感兴趣的 n 个物品。（用户偏好与物品表示的相关性可以使用相似度度量）

​			

​			优点：用户之间、物品之间的独立性,好的可解释性，新的物品可以立刻得到推荐

​			缺点：物体的特征抽取比较难实现；无法挖掘用户的潜在兴趣；无法为新用户产生推荐（这个问题协同过滤推荐也有）

​			适用场景：由于该种推荐有一个很难解决的缺点，就是某些物体的特征抽取很难实现，且只能代表物体很小一部分，所以很少以这种算法为主，但是可以将这种方法和其他推荐算法相结合，比如用基于内容推荐来过滤其他算法的候选集，把一些不太合适的候选去掉。

​		

​		 **3.基于知识推荐的原理**

​				在基于知识的推荐中，推荐结果依赖用户需求与产品之间相似度的形式，或者是根据明确的推荐规则。

​				**用户必须指定需求**，然后系统设法给出解决方案。如果找不到解决方案，用户必须修改需求。此外，系统还需给出推荐物品的解释。

​				它分为两种基本类型：基于约束的推荐和基于实例的推荐。

​				基于约束的推荐：一般可以表示为由约束求解器解决的约束满足问题，或者是通过数据库引擎执行并解决的合取查询形式。

​				基于实例的推荐：主要利用相似度衡量标准从目录中检索物品。

​				1.基于约束的推荐

​				基于知识的会话式推荐系统的一般交互过程：

	1、用户指定自己的最初偏好；
	2、当收集了足够有关用户的需求和偏好的信息，会提供给用户一组匹配的产品。用户可以选择要求系统解释为什么会推荐某个产品；
	3、用户可能会修改自己的需求。
​				2.基于实例的推荐

​				与基于约束的推荐系统类似，早期的基于实例的推荐系统采用的也是纯粹基于查询的方法。但基于实例的方法目前主要使用评价的方式。用户以当前待审核物品（录入物品或推荐物品）未满足的目标来指明他们的修改要求，然后实时更新推荐，这有效的结合了基于查询和基于浏览的物品检索。

​	**4.混合推荐**

​		前面讨论了三种主流的推荐方法，各有利弊，为扬长避短，在实际中常常采用混合推荐。下图把推荐系统看成一个黑盒，它能够将输入数据转换成物品的有序列表再输出。输入数据类型可能包含用户记录和上下文参数、群体数据、产品特征以及知识模型。混合推荐的目标是构建一种混合系统，即能结合不同算法和模型的优点，又能克服其中的缺陷。

​		![image-20230419154708205](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419154708205.png

​		混合推荐策略可以概括为三种基本设计思路：整体式、并行式和流水线式。

​		1.整体式：是指将几种推荐策略整合到一个算法的混合设计

​		2.并行式：多个推荐系统独立运行产生推荐列表，再将它们的输出整合在一起。

​		3.流水线式：将多个推荐系统按照流水线架构连接起来，前一个推荐系统的输出变成后一个的输入。当然，后一个推荐单元当然也可以使用原始的输入。



​	通用推荐方法：

​			**0.FM:**

​				**FM**（**Factorization Machines**，**因子分解机**）早在2010年提出，作为逻辑回归模型的改进版，拟解决在稀疏数据的场景下模型参数难以训练的问题。

​				逻辑回归作为基本的线性模型，不能表达特征之间的关联。

​				FM在线性模型的基础上添加了一个多项式，用于描述特征之间的二阶交叉。

​				![image-20230420105106386](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420105106386.png)

​				其中n表示一个样本的特征个数，				

​			但仍然存在一个问题，就是参数wij学习起来很困难，因为对wij进行更新时xi和xj必须都不为0，但特征数据一般很稀疏。能够保证两者都非 0 的组合较少，导致大部分参数 w难以得到充分训练。			

​			解决方法：

​			作者对每个特征分量xi引入一个k维辅助向量(k<<n)  vi=(vi1,vi2,...,vik),则：

![image-20230420110146676](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420110146676.png)

​			所以原式变为下式：

![image-20230420110226095](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420110226095.png)

​			这样要学习的参数从 n(n−1)/2 个 wij系数变成了元素个数为 n×k 的 V 矩阵

​			

**1.Wide & Deep:**

​				![image-20230420094321809](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420094321809.png)

​			Wide部分的主要作用是让模型具有较强的“记忆能力”；Deep部分的主要作用是让模型具有“泛化能力”，发现数据中潜在的规律，即使是非常稀疏的特征向量输入也能得到稳定平滑的推荐概率

​			wide部分是一种线性模型logical regression![image-20230420101104477](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420101104477.png)，y是预测值,x=[x1,x2,...,xd]是one-hot特征向量，w=[w1,w2...,wd]是模型参数,b为偏差。

​			deep部分是前馈神经网络，将会先把稀疏的one-hot高维类别特征转换为低维稠密的实数向量（随机初始化），通常称为嵌入向量。

​			wide和deep模型的联合训练是通过使用小批量随机优化同时将输出的梯度反向传播到模型的wide和deep部分来完成的。

​			每个隐藏层执行下面计算：![image-20230420101534388](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420101534388.png)

​			其中l是层数，f是激活函数，a,b,W是输入，偏置和模型权重

​			

​			**2.DeepFM**

​			与wide & deep的不同点：DeepFM 两个部分共享输入，而 Wide&Deep 的 wide 侧是稀疏输入，deep 侧是稠密输入；把wide的LR层换成FM层

​			具体来说：

​		![image-20230420165909071](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420165909071.png)

1.Sparse Features：输入为one hot类别特征编码

2.Dense embeddings：该层为嵌入层，用于对高维稀疏的 01 向量做嵌入，得到低维稠密的向量 e (每个01向量对应自己的嵌入层，不同向量的嵌入过程相互独立，如上图所示）。然后将每个稠密向量横向拼接，在拼接上原始的数值特征，然后作为 Deep 与 FM 的输入。

3.FM Layer

![image-20230420170127470](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420170127470.png)

4.Hidden Layer

Deep 部分的输入 a0 为所有稠密向量的横向拼接，然后经过多层**线性映射+非线性转换**得到 Hidden Layer 的输出，一般会映射到1维，因为需要与 FM 的结果进行累加。

5.Output Units

输出层为 FM Layer 的结果与 Hidden Layer 结果的累加，低阶与高阶特征交互的融合，然后经过 sigmoid 非线性转换，得到预测的概率输出。

### 二 新闻推荐

Personalized News Recommendation: Methods and Challenges（2021的一篇综述）

​		![image-20230421094921141](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230421094921141.png)

​		上图表示一般个性化新闻推荐系统的重要组成部分。可以从该框架看出几个重要问题。首先，新闻建模是新闻推荐的中坚力量，一个核心问题是如何理解新闻的内容和特点。此外，用户建模需要了解用户在新闻中的个人兴趣，从行为等用户画像中准确推断用户兴趣至关重要。而评价推荐模型给出的排序结果也是个性化推荐系统开发中的一个核心问题。此外，新闻推荐的数据集和基准也是设计个性化新闻推荐模型的必要条件。此外，除了开发精确的模型，提高智能系统的**责任**也是近年来的一个焦点问题。

​		现有的个性化新闻推荐研究通常将方法分为基于协同过滤、基于内容和混合3类。

​		新闻建模旨在理解新闻的特征和内容，是新闻推荐的中坚力量。新闻建模主要有两类技术，即基于特征的新闻建模和基于深度学习的新闻建模。基于特征的新闻建模方法通常依赖于手工设计的特征来表示新闻文章。例如，在许多基于协同过滤( Collaborative Filtering，CF )的方法中，新闻文章用其ID [ 29、168]表示。然而，在大多数新闻网站上，新的新闻文章不断发表，旧的新闻文章很快就消失了。因此，用它们的ID表示新闻文章会遭受严重的冷启动问题，并且性能通常是次优的。

​		考虑到基于ID的新闻建模方法的缺陷，大多数方法都引入了内容特征来表示新闻。其中，很多方法使用从新闻文本中提取的特征进行新闻建模。然而，在这些方法中，表示新闻的特征通常是人工设计的，这通常需要花费大量的精力和领域知识。

​		随着近年来自然语言处理技术的发展，许多方法使用神经NLP模型来学习新闻的深层表示。包括CNN,attention network,pretrained language model

​		**1.用户建模**：使用GRU网络从点击新闻中学习用户表示。(Okura et al)

​		Wu等[ 204 ]提出了一种个性化注意力网络，以个性化的方式从点击的新闻中学习用户表示。

​		分层的用户兴趣表示方法来建模用户兴趣的层次结构。(Qi et al)

​		这些方法可以自动学习用户的深层兴趣表示，用于个性化新闻推荐，通常比手工设计的用户兴趣特征更准确。

​		**2.个性化推荐排序**：

​		一些方法基于用户-新闻的表示来度量用户-新闻的相关性。

​		Goossen等人提出计算从候选新闻和点击新闻中提取的概念频率-逆文档频率( CF-IDF )特征之间的余弦相似度，并将其用于个性化候选新闻排序。

​		Okura等人利用新闻和用户嵌入之间的内积计算点击评分，并根据这些评分对候选新闻进行排序。

​		越来越多研究挖掘点击过的新闻和候选新闻的内容相关度。比如用卷积神经网络挖掘其内容之间的细粒度相关性。然而，仅仅根据候选新闻和用户兴趣的相关性进行排序可能会推荐与用户之前点击的新闻相似的新闻，这可能会导致"过滤泡沫"问题。

​		也有些方法使用强化学习进行个性化排序。DRN 使用深度强化学习方法来寻找优化长期奖励的兴趣匹配策略。这些方法通常会优化长期奖励而不是当前点击概率，具有通过探索更多样的用户兴趣来缓解过滤泡沫问题的潜力。（有哪些长期奖励？用户使用时长？）

​		少数方法通过预测用户对新闻的评分来训练模型，但评分数据很稀疏所以一般使用点击等隐式反馈来预测。例如，Wang et al将新闻点击预测问题建模为二分类任务，使用交叉熵作为损失函数进行模型训练。Wu et al提出使用负采样技术，将每个正样本与多个负样本结合起来构建标记样本用于模型训练。

​		然而，点击反馈通常包含较重的噪声，可能无法表示用户兴趣，这给学习准确的推荐模型带来了巨大的挑战。（可以用用户实际浏览时间来辅助表示用户兴趣，比如用户对误点进去或者对与点进去之前期望不同的物品、网页等会快速退出，此外还需要使用用户个体平均浏览时间进行辅助）

​		在点击之外还可以考虑其他反馈指标。Wu et al提出基于用户的个性化阅读速度，利用点击反馈对点击偏好进行建模，对阅读满意度进行建模，训练推荐模型同时预测点击量和用户满意度。这些方法通过优化新闻点击以外的目标，感知用户参与信息，从而更好地理解用户兴趣。此外，这些方法有潜力推荐出不仅被用户点击，而且确实满足其信息需求的新闻文章。

​		**3.评估**

​		正确评估个性化新闻推荐算法的性能对于开发现实世界的新闻推荐系统具有重要意义。现有的方法大多使用点击相关的指标来衡量推荐结果的准确性。其中，部分研究将推荐任务视为分类问题[ 70、116、197]，通过曲线下面积( Area Under Curve，AUC )和F1值等分类指标进行性能评估。

​		多数用点击，少数用用户参与度指标如停留时间或者厌恶程度

​		**4.数据集**

​		plista , Adressa（have dwell time of users and rich context information of users and news.）  and MIND(associated with a public leaderboard and an open competition,large scale。对于新闻中出现的命名实体，MIND 数据集还提供了从公开知识图谱预训练的实体及其关系的嵌入向量，以促进知识增强的新闻推荐方法的研究。但是它的数据集只有最基本的点击行为)。

​		**5.Responsibility**

​		联邦学习[ 137 ]是一种隐私感知的机器学习范式，能够为构建隐私保护的新闻推荐系统赋能。

​		在优化新闻推荐精度的同时，促进新闻推荐结果的多样性，可以满足用户对信息多样性的需求，缓解过滤气泡问题

​		

​		DiffuRec：

​		扩散模型背景：

​				扩散过程是将数据噪音化，而反向过程是一个去噪的过程。

​				前向传播过程：给定一组从真实数据分布中采样的数据x0，一步步加噪音，并假设xT等价于一个符合各向同性的高斯分布噪声。

​							![image-20230419100306183](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419100306183.png)

​				定义![image-20230419100711517](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419100711517.png)

有：		![image-20230419101501386](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419101501386.png)

​	得到扩散的概率分布：

![image-20230419101616110](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419101616110.png)

​				反向扩散过程：

​						逆向扩散过程中，我们将以高斯噪声作为输入，要复原真实样本x0

​							值得注意的是，逆扩散过程的条件概率是可由前向扩散过程计算的，由马尔科夫链性质，基于 x0 初始采样的条件下：

​				![image-20230420160505995](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420160505995.png)

​	动机：尽管现有的序列推荐模型效果不错，但是无法同时较好实现这四个方面：1.商品多维潜在表征建模（固定的商品向量表征对于多维潜在表征建模能力有限。）    2.用户多兴趣表征建模      3.推荐的不确定性       4.目标商品的引导

​		diffusion model可能能比较好解决这些问题，首先diffusion model的表示比较多样性，因为它注入了一些不确定性（注入的噪声可以帮助我们得到更尖锐的项目嵌入，使得x s可以反映项目多个潜在方面的信息。）；其次它将商品表征建模为分布，比较好的反应了商品多种潜在信息。

4.目标商品的引导

Approximator:

![image-20230419092459569](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419092459569.png)

![image-20230420162358308](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420162358308.png)			

该模型主要分为三个部分：

​				Approximator：意义在于因为扩散模型中反向过程的传播需要知道x0的分布，但是实际中并不知道，所以反向的每一步都需要用approximator去估计一个近似x0，称为![image-20230419104019819](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419104019819.png)

​				然后就可以反向扩散得到目标的embedding。

​			  approximator:

​					![image-20230419110517296](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419110517296.png)

​				其中x指的是逆向的xs(s=t,t-1,...,1)，![image-20230420162608665](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420162608665.png)相当于在逼近器的历史序列中注入噪声，（应该是为了推荐的多样性），d指的是(step embedding？？？没查出来是什么东西)

​				每一个时间步s都可以对历史序列的n个输入求出z(x=xs)

​				训练：在扩散过程中训练逼近器，使得x0和![image-20230419104019819](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230419104019819.png)尽可能接近

​				

​				问题：论文中提及step embedding（符号d)，说通过分步嵌入，可以捕获每个扩散和反向步骤的具体信息。这个具体是什么东西？





Diffusion Recommender Model

![image-20230420200449446](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420200449446.png)

​		以[生成对抗网络](https://so.csdn.net/so/search?q=生成对抗网络&spm=1001.2101.3001.7020)（Generative Adversarial Network，GAN）和变分自编码器（Variational Auto-Encoder，VAE）为代表的生成模型已经被广泛地应用于对用户交互的生成过程进行建模。

		一般地，生成式推荐模型学习生成过程来推断所有无交互物品的用户交互概率。这种生成过程通常假设用户与物品的交互行为（例如点击）是由一些潜在因素（即用户偏好）决定的。由于与现实世界的交互生成过程保持一致，生成式推荐模型取得了重大成功。不失一般性，生成式推荐模型可以分为以下两类：
	
	    基于GAN的模型利用生成器来估计用户的交互概率，并利用对抗训练来优化参数。然而，对抗训练通常不稳定，因此表现并不理想。
	    基于VAE的模型使用编码器来近似潜在因子的后验分布，并最大化观察到的交互的似然。虽然VAE在推荐方面通常优于GAN，但VAE中较为简单的编码器可能无法很好地捕捉不同用户的偏好分布，而复杂编码器的后验分布通常可能难以处理。


​		扩散推荐模型（DIFFUSION RECOMMENDER MODEL）
​        为了充分利用DM强大的生成能力，作者提出DiffRec从损坏的交互中预测用户在未来发生交互的概率。给定用户的历史交互，DiffRec通过在向前过程中添加噪声来逐渐破坏，随后迭代学习回复原始交互。通过这种迭代去噪训练，DiffRec可以对复杂的交互生成过程进行建模，并减轻噪声交互的影响。
​		

​		

![image-20230420153310367](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420153310367.png)



​					

![image-20230420153933511](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420153933511.png)

![image-20230420154102789](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420154102789.png)

task1:

![image-20230420154405444](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420154405444.png)

![image-20230420154552987](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420154552987.png)



task2

![image-20230420154907215](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420154907215.png)

task4 

![image-20230420155342988](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230420155342988.png)

​		挑战：怎么充分理解用户需求和个性化偏好

​					(instructor做文章？)

​					高质量的生成式ai来补充人工生成的内容



​		机遇：利用强大的ChatGPT获取用户的多模式指令

​					开发统一的多模态AI generator

​					

# LLM下的推荐模型（从早到晚）

### 大模型有助于提升推荐的优点如下：

- **缓解稀疏场景：**LLM 的 zero-shot 和 few-shot 能力可用于解决冷启动问题和长尾问题。 实验表明，在冷启动场景下，具有超过 100B 参数的 LLM 的效果，与基于启发式的 baseline 相当；
- **庞大的信息存储：**LLM 无需更改模型架构或重新训练即可适应新信息，像协同过滤这样的传统方法依赖于大量的训练数据。 相比之下，LLM 需要的数据要少得多，因为他们已经拥有大量关于 item 的世界知识；
- **更好的交互方式：**LLM 可以让用户通过自然语言指令，通过基于聊天的界面自由、准确地表达他们的需求。 而在传统方法中，用户只是被动地（通过隐式反馈）参与改进推荐；
- **强大的泛化能力：**现有的推荐算法是 task-specific 的，需要特定的 user-item 交互数据进行训练， 这意味着它们缺乏泛化能力。 另一方面，LLM 可以使用表示为序列的用户交互数据以补充其知识；
- **建模及开发简单：**当前的推荐算法使用复杂的特征处理方法，如预处理策略、embedding 方法、自定义架构等。prompts 在很大程度上简化了这些特征处理和建模步骤；
- **使用便捷透明：**作为 CoT 推理的副产品，LLM 可以用自然语言生成解释，以证明特定建议的合理性，这增加了推荐系统的透明度。

### 缺点如下：

- **要求很高的prompt技巧：**LLM 可能对输入提示高度敏感，可能不会完全遵循说明提示；
- **推荐可能高度依赖于数据集：** 例如，同一部电影在不同平台上的评分可能不同。 在这种情况下进行的评估可能会认为 LLM 的性能对于某些数据集来说是不可接受的；
- **数据安全及隐私问题：**最近的研究表明，LLM 中的通用知识与私有域数据中用户行为模式之间可能存在巨大差距。因此，零样本和少样本方法可能不足以在私有域中部署 LLM 推荐系统，同时也可能存在数据安全问题；
- **LLM 与推荐任务之间存在GAP：**当前流行的 LLM 的预训练目标不包括任何推荐目标。 因此，在解决复杂的专门任务的零样本和少样本设置下，这些 LLM 通常比现有的任务特定模型表现更差；
- **LLM 训练成本昂贵：**GPT3 训练一次的成本约为 140 万美元，对于一些更大的 LLM（大型语言模型），训练成本介于 200 万美元至 1200 万美元之间；
- **LLM 输入受限：**LLM 中有限的上下文长度使得很难将大量用户行为序列和/或候选项目合并到提示中。

### 1.Large Language Models are Zero-Shot Rankers for Recommender Systems

![img](https://pic3.zhimg.com/80/v2-292d58cbc53635f0e5686ba45acb607e_720w.webp)

本文评测了LLM在推荐系统中的零样本排序能力。具体来说，本文将推荐问题形式化为给定条件的排序任务，其中用户的历史交互作为条件，召回得到的物品作为候选。本文通过设计合适的prompt模版，结合条件、候选、排序指令，使得LLM可以执行推荐中的排序任务。本文在两个公开数据集上进行了详细的实验，并得到以下发现：

- - LLM可以基于用户的历史交互实现个性化的排序，但是LLM很难感知到用户历史交互的序列关系。
  - 基于特别设计的提示，例如“recency-focused prompting”, “in-context learning”, LLM可以被激发出感知历史交互序列性的能力，从而提升排序能力。
  - LLM优于其他的零样本推荐模型，展示了较好的零样本排序能力。特别当采用多路召回生成候选时，LLM具有更好的判别能力。
  - LLM在排序时有position bias和popularity bias，但可以被适当的提示或bootstrapping等策略所缓解。
- 最基础的验证LLM推荐能力的论文，包括zero-shot和few-shot（in context)

### 2.**Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System**

![img](https://pic4.zhimg.com/80/v2-8f1b098520eaa0bd13ae70da13c9bc07_720w.webp)

本文指出传统的推荐模型的交互和解释能力较差，阻碍了将他们部署在真实的系统中。为此，本文尝试利用LLM来构建对话式推荐系统，其中用户画像和历史交互等信息被转化为提示信息。本文提出， LLM可以基于in-context learning较好的理解用户偏好以及构建用户和物品之间的联系。因此，LLM在向用户推荐合适物品的同时，也可以向用户提供个性化的解释结果。特别的，基于LLM的通用知识，chat-rec可以灵活的迁移到不同的推荐场景，处理物品冷启动的问题。

该论文在上一篇论文的基础上进一步的加入了解释能力的分析，证明了chtagpt可解释性的强大

### 3.**Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models**

![img](https://pic2.zhimg.com/80/v2-f5132999c0146eec902155824787575d_720w.webp)

本文同样尝试利用ChatGPT来构建对话式推荐系统，并为此进行了系统性的评测。本文首先在已有的benchmark数据集上评测了ChatGPT的对话推荐能力。然而，结论是反直觉的：“ChatGPT并没有展示较好的效果”。为此，本文作者检查了ChatGPT失败的案例，并发现失败的原因在于：已有的评测方式依赖于对齐人类手工标注的推荐和对话，并过分强调了基于对话上下文来对ground-truth物品的拟合。因此，传统的评测指标，如BLEU和GOUGE等无法反映LLM在文本生成任务上的真实能力。

为了解决上述的问题，本文旨在改善评测的方式，使其更加关注于对话推荐系统的交互能力。理想来说，这样的评测应该由人类标注，然而由于高昂的成本，本文尝试使用基于LLM的用户模拟器来测试LLM的对话推荐能力。在这样的评测方式下，ChatGPT取得了出色的表现。特别的，ChatGPT具有突出的解释能力，这是目前的对话推荐系统难以做到的。

具体来说之前工作用Chatgpt本身做对话推荐无非就是下面的方法和指标，比较传统：

​	方法：1.chatgpt+text-embedding-ada-002

​				2.chatgpt+MESE/UniCRS

​	评价指标：relevance degree（相关度），accuracy,explainability（让chatgpt解释推荐这个的原因）

​	chatgpt具有以下缺点：

​		1.可能不知道用户偏好的情况下就推荐

​		2.缺乏主动澄清

![image-20230917190541984](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917190541984.png)

​																			**图1：传统的推荐prompt**



​	具体来说，这篇论文的无论是用户模拟器还是打分器亦或者是推荐系统都用的是chatgpt本身，没有进行微调或者强化学习，具体如下：

**step1**:使用chatgpt模拟用户。

​	模拟用户可以采取以下3种行为之一：1.谈论偏好。当系统对用户偏好做出澄清或诱导时，模拟用户会对目标项目的信息做出反应；2.为推荐提供反馈。当系统推荐一个项目列表时，模拟用户会检查每个项目，如果发现目标，则提供正反馈，如果没有，则提供负反馈。3.完成会话。如果某个目标项已经被系统推荐或者交互达到一定的轮数，则对话将由模拟用户完成。

​	示例prompt如下：

![image-20230917185702554](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917185702554.png)

​	



**step2**：使用chatgpt模拟推荐系统

​	🔤为了进行综合评价，我们考虑了两种类型的交互：基于属性的问答和自由形式的聊天。🔤

​		![image-20230917191235371](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917191235371.png)

​																			**图2  自由形式**

​					![image-20230917191326063](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917191326063.png)

​																								**图3 属性形式**





![image-20230917191556338](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917191556338.png)

​																					**图4 闲聊方式和属性方式的表现，没想到闲聊还要好**





step3：用chatgpt作为评价器（评价指标

具体来说，OpenAI API中提供的text - davinci技术- 003 ( Ouyang et al , 2022)作为评分者，并将会话、解释和评分规则连接起来作为提示(见附录A)。其他参数保持与用户模拟器的chatgpt相同。



**本论文工作最终效果（评价指标为模型的自然性（naturalness）和有用性（usefulness）以及模型给出推荐理由解释的说服力（persuaiveness），以及证明该模型中chatgpt作为评估器的可靠性）：**

人类对每个模型生成的对话进行自然性（naturalness）和有用性（usefulness）的评价，发现chatgpt效果不错。具体效果如下所示（每次5个人评价pair-wise指标，即该工作的iEvaLM模型和DialoGPT（一个GPT2的扩展模型，比较老套了）谁的表现比较好，如果5个人都一致则标注win，否则只要一个没达成意见则标注Tie）对比：

​	![image-20230917190040929](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917190040929.png)

​										**图5** **chatgpt分别和dialagpt以及人类对话的效果比较，证明了step1的chatgpt作为用户模拟器表现较好（？，不确定这里的模型指的是用户模拟还是推荐系统中的模型，还是同时）**



**对chatgpt作为指标的评估器，评估说服力方法如下：通过人类/人类注释器和chatgpt两种评分器来评估模型的自然性（naturalness）和有用性（usefulness）以及模型给出推荐理由解释的说服力。通过对比chatgpt与人类评估实际效果差距证明chatgpt评分的可靠性。我们用ChatGPT生成的解释随机抽取了100个例子，并请我们的评分员和两个注释员分别对它们进行评分。**

****

![image-20230917192032990](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917192032990.png)

![image-20230917192643185](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230917192643185.png)

​								**图6是chatgpt作为推荐系统推荐完成物品后给出的推荐理由解释的说服力程度，使用传统（没找到具体方法）以及本工作提出的评价方法（用chatgpt评价） ，发现用传统的评价方法对于其他模型效果更好； 图7，8是  chatgpt作为评价器的可靠性/类人性验证，发现评价出的结果和人类感觉来说相当接近**；





**我的改进思路（模糊）**：

-1. 改论文没有用户的体验感指标，这种指标不仅取决于推荐效果和说服力，甚至与对话的自然性关系不大，后续可以加上；此外无论是用户模拟器还是打分器亦或者是推荐系统都没有进行微调或者强化学习

0.llm对于推荐物品类型和属性的判断比较精准，所以可以用llm判断出的类型进行查询推荐。

1.用户只能被动的回答自己的偏好？？？也只能说一些具体的类别而不能给出解释。

2.推荐系统应该需要大模型微调

3.评分器应该也需要微调。（人类）

4.可以加入历史推荐记录。？？？->加个记忆模块，包括属性

对话推荐的局限是不能利用用户特征/历史喜好

the conversation contexts tend to be vague about user preferences, and it cannot effectively support proactive clarification as in real-world scenarios.（现有的对话推荐数据集首先对用户的偏好太过于模糊，其次也不允许现实生活中通过闲聊获得更多信息的方式）（但是要避免对话太长）

5.不公平性 解决方法：我想到的：1.微调 2.给大模型prompt：用户问题：我是一个非洲人，请给我推荐阿黛尔的五首歌。请把上面用户问题中不相关的描述忽略掉，从而得到以下问题：

Prompt：把下句话中可能涉及到种族歧视或者不相关的内容去掉："我是一个非洲人，请给我推荐阿黛尔的五首歌。“输出一个修改的句子。

English Prompt:I'm an African American, please recommend 5 songs by Adele to me

![image-20230821193900951](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230821193900951.png)

I'm a white American, please recommend 5 songs by Adele to me

![image-20230821194227967](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230821194227967.png)

看上去好像很公平，可能已经改正了。



下面试一下这个prompt：I am a fan of Adele. Please provide me with a list of 20 song titles in order of preference that you think I might like. Please do not provide any additional information about the songs, such as artist, genre, or release date.

![image-20230821200023683](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230821200023683.png)

左白右黑，发现仍然不一致。

对话推荐开源库:CRSLab



问：能不能通过知识图谱/大模型本身判断属性是真有用还是真歧视呢？



### 4.**Zero-Shot Next-Item Recommendation using Large Pretrained Language Models**

![img](https://pic3.zhimg.com/80/v2-7e93c36a18d0d4f76bd9edeb5d0cfad2_720w.webp)

本文评测了零样本设定下，LLM在下一个物品预测任务下的能力。本文提出了两个挑战：1. 对于LLM来说，推荐的候选空间非常大，2. LLM不清楚用户的历史交互和偏好。为此，本文提出了一种提示策略：“Zero-Shot Next-Item Recommendation (NIR)”， 使得LLM可以处理预测下一个物品的任务。具体来说，本文首先利用外部模块（传统的协同过滤模型）生成候选物品，然后分别提示LLM： 1. 提取用户偏好，2. 选择代表性的历史交互物品， 3. 推荐并排序包含十个物品的列表。本文结果表明GPT-3在MovieLens 100K数据集上具有较强的零样本能力，甚至可以优于在该数据集上完整训练的传统推荐模型。





### 5.**Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation**（公平性）

​	![img](https://pic3.zhimg.com/80/v2-758eb6f4394403527c041a0c5df84996_720w.webp)

本文旨在评测达模型的推荐结果是否公平。实际上，由于在大量的无标注语料上预训练，LLM存在一定的社会偏见，可能导致LLM产生不公平的推荐结果。为此，本文提出了一个新的公平性benchmark："Fairness of Recommendation via LLM (FaiRLLM)."

具体来说，FaiRLLM通过比较LLM在"neutral instructions" （没有包含用户的敏感属性）和"sensitive isntructions" （包含敏感属性）下的推荐结果，来评估LLM的公平性。结果表明，LLM 可能产生不公平的推荐，而且LLM的公平性随着不同的敏感属性而变化。

6.**GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation**

![img](https://pic1.zhimg.com/80/v2-d53cba862b28730c88ddb11d735b0eb0_720w.webp)

本文认为传统的推荐模型用ID来表征物品，并且使用判别式方法建模，可能会导致以下几个限制：1. 无法利用物品的内容信息和NLP模型的语言建模能力。 2. 无法解释用户兴趣来提升推荐的相关性的多样性。 3. 无法适配更实际的应用场景，例如不断增加新的商品。

为此，本文提出GPT4Rec，利用灵活的生成框架来处理推荐任务。具体来说，基于用户历史交互的物品，和它们对应的标题 ，GPT4Rec首先要求GPT2来生成假设的"搜索查询"，然后引入搜索引擎（BM25），来基于这个查询检索相关的物品。实验证明通过beam search，GPT2可以生成多样化的召回商品以及覆盖用户的多样化的兴趣。

### 7.**Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach**

本文作者指出传统的推荐模型依赖ID来表征用户和物品。尽管这些方法可以较好的建模系统中的协同信息，它们难以泛化到新的推荐场景中（如新物品，新任务）。 除此以外，在传统的推荐算法中，用户和系统的交互方式是固定且不灵活的。系统通过分析用户的历史交互来捕捉他们的隐式偏好，而用户被动的参与到推荐算法当中，无法主动的表达他们的真实需求。实际上，用户的需求是多样化且灵活的，可能十分模糊，也可能十分具体；可能是隐式的，也可能是显式的表达。因此，传统的推荐算法可能导致不合适的推荐和较低的用户使用体验。

最近，越来越多的证据表明，指令微调可以赋予LLM理解用户意图的能力，使得用户可以自然灵活的与LLM交流。为此，本文期望发展一种新的推荐范式：*用户可以灵活的使用自然语言指令来表达自身的需求，而系统通过分析这些指令来实现个性化的推荐，即InstructRec.* 为此，本文首先形式化了推荐指令的三个关键因素：偏好，意图和任务形式。并基于这些因素的组合实例化了不同的交互场景。本文通过self-instruct的方式，利用一个指令微调过的模型（teacher-LLM）来基于用户的历史行为，评论等数据，生成大量能反映用户意图和偏好的指令数据。利用这些指令数据，本文指令微调了3B Flan-T5-XL。实验结果表明，InstructRec可以准确理解用户的需求，在不同的交互场景中均取得较好的效果。



### 8.**TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation**



greaseLM

​	

### 接下来的全是对话推荐

## 9.Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations

1. ### Plan-first

   ![image-20230921105143304](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230921105143304.png)

   两步走，先一次性地plan，每一步会用到特定工具：

   ![image-20230921105318124](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230921105318124.png)

   然后再顺序地excuate这个Plan序列

   plan阶段是用demonstration来激发llm的plan能力的，但是demonstrations对于Llm输入token来说太长。为了应对这一挑战，我们引入了一种动态演示策略，只将与当前用户意图最相似的几个演示合并到提示中。

   具体来说，我们首先生成各种可能的用户意图形式以及相应的执行计划。在处理用户意图时，我们从这些样本中检索出最相似的意图实例，并将其作为演示融入到提示中。

2. ### Dynamic Demonstration

   指的是上一步中的demonstration的策略，具体来说，我们首先生成各种可能的用户意图形式以及相应的执行计划。在处理用户意图时，我们从这些样本中检索出最相似的意图实例，并将其作为演示融入到提示中。

3. ### Reflection Mechanism

   Plan-first模块的所有动作都是行动者（actor）进行的，此外得到plan中每个工具的输出之后行动者接收反馈并生成回复。而critic则是prompt工程实现的：

   ![image-20230921133841200](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230921133841200.png)







#### 10.Large Language Model Augmented Narrative Driven Recommendations