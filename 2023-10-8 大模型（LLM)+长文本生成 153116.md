

# 1.RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning

- 没有在大模型，如 GPT-3 上进行实验。
- 与典型的RL方法一样，设计 reward functions 可能需要领域的专业知识。然而，可以使用 **inverse RL** 等技术来解决这个问题，它从数据中学习 reward functions。（奖励函数是怎么设计的？）
- 就跨模型的可转移性而言，目前还没有仔细研究学习 prompts 的模式，或所谓的“**秘密语言**”。藏状态中。”）

2.MEMORIZING TRANSFORMERS

比第一篇早，是第一篇的基础之作，普通的外部key-value存储库，但是使用近似knn在里面搜索相似的然后拿来和当前query求，同时当前段的query当然也会和当前段的key value求，最后融合。



大语言模型的长文本生成现在除了用这个外部注意力方法，滑动窗口方法还有什么其他方法？该任务还有什么问题没有解决？



